#!/usr/bin/env python3
"""
æµ‹è¯• Ollama æœ¬åœ° LLM API
"""
import requests
import json
import time

OLLAMA_API_URL = "http://localhost:11434/api"

def test_ollama_models():
    """åˆ—å‡ºå¯ç”¨çš„ Ollama æ¨¡å‹"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])
            print("=" * 60)
            print("ğŸ“¦ å¯ç”¨çš„ Ollama æ¨¡å‹ï¼š")
            print("=" * 60)
            for model in models:
                name = model.get("name", "unknown")
                size = model.get("size", 0) / (1024**3)  # è½¬æ¢ä¸º GB
                print(f"  â€¢ {name} ({size:.2f} GB)")
            return [m.get("name") for m in models]
        else:
            print(f"âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Ollama æœåŠ¡å™¨æœªè¿è¡Œæˆ–æ— æ³•è¿æ¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨: ollama serve")
        return []


def test_ollama_chat(model_name="llama3.2", prompt="ä½ å¥½"):
    """æµ‹è¯• Ollama èŠå¤©åŠŸèƒ½"""
    print("=" * 60)
    print(f"æµ‹è¯• Ollama æ¨¡å‹: {model_name}")
    print("=" * 60)
    print(f"æç¤ºè¯: {prompt}")
    print()
    
    try:
        start_time = time.time()
        
        response = requests.post(
            f"{OLLAMA_API_URL}/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            elapsed = time.time() - start_time
            
            print("âœ… å“åº”æˆåŠŸï¼")
            print(f"è€—æ—¶: {elapsed:.2f} ç§’")
            print()
            print("å“åº”å†…å®¹:")
            print("-" * 60)
            print(result.get("response", ""))
            print("-" * 60)
            print()
            
            return {
                "success": True,
                "response": result.get("response", ""),
                "time": elapsed
            }
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(response.text)
            return {"success": False, "error": response.text}
            
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


def test_time_analysis(model_name="llama3.2"):
    """æµ‹è¯•æ—¶é—´åˆ†æåŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•æ—¶é—´åˆ†æåŠŸèƒ½")
    print("=" * 60)
    
    transcript = "æˆ‘åˆšåˆšåƒå®Œé¥­äº†"
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ—¶é—´è®°å½•åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šé€šè¿‡è¯­éŸ³è¾“å…¥æ—¶é—´ä¿¡æ¯ï¼Œä½ éœ€è¦ä»æ–‡æœ¬ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. æ´»åŠ¨åç§°ï¼ˆactivityï¼‰ï¼šç”¨æˆ·åœ¨åšä»€ä¹ˆ
2. å¼€å§‹æ—¶é—´ï¼ˆstart_timeï¼‰ï¼šæ´»åŠ¨çš„å¼€å§‹æ—¶é—´ï¼ˆISO 8601 æ ¼å¼ï¼‰
3. ç»“æŸæ—¶é—´ï¼ˆend_timeï¼‰ï¼šæ´»åŠ¨çš„ç»“æŸæ—¶é—´ï¼ˆISO 8601 æ ¼å¼ï¼‰
4. æŒç»­æ—¶é—´ï¼ˆduration_minutesï¼‰ï¼šå¦‚æœæåˆ°äº†æ—¶é•¿ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿæ•°

æ³¨æ„ï¼š
- å¦‚æœç”¨æˆ·è¯´"åˆšåˆš"ã€"åˆšæ‰"ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
- å¦‚æœç”¨æˆ·è¯´"æ¥ä¸‹æ¥"ã€"æ‰“ç®—"ï¼Œè¿™æ˜¯æœªæ¥çš„æ´»åŠ¨ï¼Œåªè®¾ç½®å¼€å§‹æ—¶é—´
- å¦‚æœç”¨æˆ·æåˆ°å…·ä½“æ—¶é•¿ï¼ˆå¦‚"2å°æ—¶"ã€"30åˆ†é’Ÿ"ï¼‰ï¼Œè®¡ç®—æŒç»­æ—¶é—´
- å¦‚æœç”¨æˆ·æåˆ°æ—¶é—´ç‚¹ï¼ˆå¦‚"9ç‚¹åˆ°11ç‚¹"ï¼‰ï¼Œä½¿ç”¨è¿™äº›æ—¶é—´ç‚¹
- å¦‚æœåªæåˆ°æ´»åŠ¨åç§°ï¼Œå‡è®¾æ˜¯åˆšåˆšå®Œæˆçš„æ´»åŠ¨ï¼Œç»“æŸæ—¶é—´ä¸ºå½“å‰æ—¶é—´

è¿”å› JSON æ ¼å¼ï¼š
{
  "activity": "æ´»åŠ¨åç§°",
  "start_time": "2024-01-01T09:00:00" æˆ– null,
  "end_time": "2024-01-01T11:00:00" æˆ– null,
  "duration_minutes": 120 æˆ– null,
  "status": "completed" | "ongoing" | "planned"
}"""

    user_prompt = f"""è¯·åˆ†æä»¥ä¸‹è¯­éŸ³è½¬å½•æ–‡æœ¬ï¼Œæå–æ—¶é—´ä¿¡æ¯ï¼š

"{transcript}"

å½“å‰æ—¶é—´ï¼š2024-01-05T17:00:00

è¯·è¿”å› JSON æ ¼å¼çš„æ—¶é—´æ•°æ®ã€‚åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    full_prompt = f"{system_prompt}\n\n{user_prompt}"
    
    result = test_ollama_chat(model_name, full_prompt)
    
    if result.get("success"):
        print(f"\nâ±ï¸ åˆ†æè€—æ—¶: {result['time']:.2f} ç§’")
        print(f"ğŸ“ å“åº”é•¿åº¦: {len(result['response'])} å­—ç¬¦")
    
    return result


if __name__ == "__main__":
    print("ğŸ” æ£€æŸ¥ Ollama è¿æ¥...")
    models = test_ollama_models()
    
    if not models:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        print("   è¯·å…ˆå®‰è£…æ¨¡å‹: ollama pull llama3.2")
        exit(1)
    
    print()
    
    # æµ‹è¯•é»˜è®¤æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæˆ– llama3.2ï¼‰
    test_model = "llama3.2"
    if test_model not in models and models:
        test_model = models[0]
    
    print(f"ä½¿ç”¨æ¨¡å‹: {test_model}")
    print()
    
    # æµ‹è¯•ç®€å•å¯¹è¯
    print("ã€æµ‹è¯• 1ã€‘ç®€å•å¯¹è¯")
    test_ollama_chat(test_model, "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    print()
    
    # æµ‹è¯•æ—¶é—´åˆ†æ
    print("ã€æµ‹è¯• 2ã€‘æ—¶é—´åˆ†æåŠŸèƒ½")
    test_time_analysis(test_model)




